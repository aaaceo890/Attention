# Attention
===========

## 博客知识
-----------
1.完全图解`RNN`、`RNN变体`、`Seq2Seq`、`Attention机制`：[知乎专栏](https://zhuanlan.zhihu.com/p/28054589) <br>
