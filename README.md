# Attention
===========

##博客知识
-----------
1.完全图解`RNN`、`RNN变体`、`Seq2Seq`、`Attention机制`：[知乎专栏]（https://zhuanlan.zhihu.com/p/28054589） <br>
