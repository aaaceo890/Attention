# Attention

## 博客知识

1.完全图解`RNN`、`RNN变体`、`Seq2Seq`、`Attention机制`：[知乎专栏](https://zhuanlan.zhihu.com/p/28054589) <br>
补充知识：[LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)<br>
